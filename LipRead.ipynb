{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b331d8bc-909d-4b74-b5f4-e530b34f7345",
   "metadata": {},
   "source": [
    "#### **Lip Reading Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e45e6-821a-4f68-832e-979f2cff351b",
   "metadata": {},
   "source": [
    "##### Install/Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b1627-8b8b-4202-bf2b-abd0f817af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Install dependencies and confirm installation with !pip list\n",
    "!pip install opencv-python matplotlib imageio gdown tensorflow-macos\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e33e7-957c-47f2-a5b2-87959273b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import os\n",
    "import cv2\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9be66e-352f-4147-b7bd-38dc8319d452",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef203f-8189-42c7-922d-0a408f81b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Downloading dataset that was made for creating lip models\n",
    "url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\n",
    "output = 'data.zip'\n",
    "gdown.download(url, output, quiet = False)\n",
    "gdown.extractall('data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d10412-6716-467b-a26b-d354d94eb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path) -> List[float]:\n",
    "    '''\n",
    "    Takes in a path to a video and returns the float values for each frame\n",
    "        Args: \n",
    "            path: str --> path to video that will be passed into model\n",
    "        Returns:\n",
    "            List of floats that represents video1\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames=[]\n",
    "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        success, frame = cap.read()\n",
    "        frame = tf.image.rgb_to_grayscale(frame)\n",
    "        #isolates the mouth --> we can also use a lip detector to isolate the mouth\n",
    "        frames.append(frame[190:236,80:220,:])\n",
    "    cap.release()\n",
    "    \n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames-mean), tf.float32)/std                 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb88359-bce7-4e62-9db6-d57ba26312c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c999f2f-e42e-491b-9b6d-3daecb21602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' '](size=40)\n"
     ]
    }
   ],
   "source": [
    "char_num = tf.keras.layers.StringLookup(vocabulary = vocab, oov_token=\"\")\n",
    "num_char = tf.keras.layers.StringLookup(vocabulary = char_num.get_vocabulary(), oov_token=\"\", invert = True)\n",
    "print(f'The vocab is: {char_num.get_vocabulary()}'\n",
    "      f'(size={char_num.vocabulary_size()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a52e8-a041-4520-8835-d87ed3924f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignments(path) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2]!='sil':\n",
    "            tokens=[*tokens,' ', line[2]]\n",
    "    return char_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef329734-0765-48a9-9dde-dbc6b48b94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    path = bytes.decode(path.numpy())\n",
    "    file_name = path.split('/')[-1].split('.')[0]\n",
    "    video_path = os.path.join('data','s1',f'{file_name}.mpg')\n",
    "    alignment_path = os.path.join('data','alignments', 's1',f'{file_name}.align')\n",
    "    frames = load_video(video_path)\n",
    "    alignments = load_alignments(alignment_path)\n",
    "    return frames, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b8b76-e12b-4f65-975c-0a5ac8510bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappable_function(path) -> List[str]:\n",
    "    return tf.py_function(load_data,[path], (tf.float32,tf.int64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b331d8bc-909d-4b74-b5f4-e530b34f7345",
   "metadata": {},
   "source": [
    "#### **Lip Reading Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e45e6-821a-4f68-832e-979f2cff351b",
   "metadata": {},
   "source": [
    "##### Install/Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b1627-8b8b-4202-bf2b-abd0f817af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Install dependencies and confirm installation with !pip list\n",
    "!pip install opencv-python matplotlib imageio gdown tensorflow-macos silence-tensorflow\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e33e7-957c-47f2-a5b2-87959273b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import gdown\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Neural Network dependencies\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfa174-6f0d-4c2a-b343-4c1632b9b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supress irrelevant tensor flow warnings --> I believe these come from the new version of tf and it tells you to ignore them\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9be66e-352f-4147-b7bd-38dc8319d452",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef203f-8189-42c7-922d-0a408f81b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# #Downloading dataset that was made for creating lip-read models \n",
    "# UNCOMMENT FOR FIRST RUN\n",
    "# url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\n",
    "# output = 'data.zip'\n",
    "# gdown.download(url, output, quiet = False)\n",
    "# gdown.extractall('data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d10412-6716-467b-a26b-d354d94eb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path) -> List[float]:\n",
    "    '''\n",
    "    Takes in a path to a video and returns the float values for each frame\n",
    "        Args: \n",
    "            path: str --> path to video that will be passed into model\n",
    "        Returns:\n",
    "            List of floats that represents video1\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames=[]\n",
    "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        success, frame = cap.read()\n",
    "        frame = tf.image.rgb_to_grayscale(frame)\n",
    "        #isolates the mouth --> we can also use a lip detector to isolate the mouth\n",
    "        frames.append(frame[190:236,80:220,:])\n",
    "    cap.release()\n",
    "    \n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames-mean), tf.float32)/std                 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb88359-bce7-4e62-9db6-d57ba26312c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c999f2f-e42e-491b-9b6d-3daecb21602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' '](size=40)\n"
     ]
    }
   ],
   "source": [
    "char_num = tf.keras.layers.StringLookup(vocabulary = vocab, oov_token=\"\")\n",
    "num_char = tf.keras.layers.StringLookup(vocabulary = char_num.get_vocabulary(), oov_token=\"\", invert = True)\n",
    "print(f'The vocab is: {char_num.get_vocabulary()}'\n",
    "      f'(size={char_num.vocabulary_size()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a52e8-a041-4520-8835-d87ed3924f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignments(path) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2]!='sil':\n",
    "            tokens=[*tokens,' ', line[2]]\n",
    "    return char_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef329734-0765-48a9-9dde-dbc6b48b94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    path = bytes.decode(path.numpy())\n",
    "    file_name = path.split('/')[-1].split('.')[0]\n",
    "    video_path = os.path.join('data','s1',f'{file_name}.mpg')\n",
    "    alignment_path = os.path.join('data','alignments', 's1',f'{file_name}.align')\n",
    "    frames = load_video(video_path)\n",
    "    alignments = load_alignments(alignment_path)\n",
    "    return frames, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b8b76-e12b-4f65-975c-0a5ac8510bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappable_function(path) -> List[str]:\n",
    "    return tf.py_function(load_data,[path], (tf.float32,tf.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e7e2b-1c97-4839-a5aa-bb4d291604ed",
   "metadata": {},
   "source": [
    "##### Tensorflow Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93d955-86e6-4e0c-9c99-380e6c4d00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.list_files('./data/s1/*.mpg')\n",
    "data = data.shuffle(500, reshuffle_each_iteration=False)\n",
    "data = data.map(mappable_function)\n",
    "#batching into groups of 2, padding 75 frames with 40 tokens\n",
    "data = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))\n",
    "data = data.prefetch(tf.data.AUTOTUNE)\n",
    "train=data.take(450)\n",
    "test=data.skip(450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6337c1-3af2-4b7f-9585-98580786ad10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16, 12,  1,  3,  5, 39,  2, 12, 21,  5, 39,  9, 14, 39,  9, 39,\n",
       "         26,  5, 18, 15, 39, 14, 15, 23,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 2,  9, 14, 39,  7, 18,  5,  5, 14, 39, 23,  9, 20,  8, 39, 21,\n",
       "         39, 19,  9, 24, 39, 14, 15, 23,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the data is now in batches of 2 alignments and 2 frames\n",
    "frames, alignments = data.as_numpy_iterator().next()\n",
    "alignments, len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c06c-c321-4ca0-bd5e-ab17b974c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.as_numpy_iterator()\n",
    "val=sample.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953dc5e-8d81-4a51-a6d8-e4e4e5103f1c",
   "metadata": {},
   "source": [
    "##### Tensorflow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f171f3-dc9e-4519-8f39-e0ca5b87c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(128,3,input_shape=(75,46,140,1),padding='same',activation='relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(256,3,padding='same',activation='relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(75,3,padding='same',activation='relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Dense(char_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2b937-6e12-460d-ae1f-400f3230713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 75, 46, 140, 128)  3584      \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 75, 23, 70, 128)  0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 75, 23, 70, 256)   884992    \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 75, 11, 35, 256)  0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 75, 11, 35, 75)    518475    \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 75, 5, 17, 75)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 75, 6375)         0         \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 75, 256)          6660096   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 75, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 75, 256)          394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 75, 256)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 75, 41)            10537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,471,924\n",
      "Trainable params: 8,471,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c1bdb-ab28-4bb1-a81d-b414daa94e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#check what our model returns\n",
    "yhat = model.predict(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f6935-8c16-4b1c-b036-a41bb9e600c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqjjqqqqqqqqqqqqqqqqqqqqqqqqqtttttttvvvvvv'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction by the model before training\n",
    "tf.strings.reduce_join([num_char(tf.argmax(x)) for x in yhat[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66be80f-7090-4ca4-8174-005f64b358cd",
   "metadata": {},
   "source": [
    "##### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d402a1-83e9-478a-9c10-2d313f22b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(epoch, lr):\n",
    "    if epoch<30:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69165ed4-53db-4e43-af8a-6d15eb285d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function from keras Automatic Speech Recognition using CTC\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7987f-f2d3-44a6-826d-1ee7f05cbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProduceExample(tf.keras.callbacks.Callback): \n",
    "    def __init__(self, dataset) -> None: \n",
    "        self.dataset = dataset.as_numpy_iterator()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
    "        data = self.dataset.next()\n",
    "        yhat = self.model.predict(data[0])\n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()\n",
    "        for x in range(len(yhat)):           \n",
    "            print('Original:', tf.strings.reduce_join(num_char(data[1][x])).numpy().decode('utf-8'))\n",
    "            print('Prediction:', tf.strings.reduce_join(num_char(decoded[x])).numpy().decode('utf-8'))\n",
    "            print('~'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8f2b4-7b87-4cd0-8504-2f3e97f8ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=CTCLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95124efa-1727-4505-a836-edb362c28e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)\n",
    "schedule_callback = LearningRateScheduler(learning_rate)\n",
    "example_callback = ProduceExample(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aad481-dadf-4d9a-afb0-d3d1331de2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data on this machine would take many days, so a pre-trained model (with the exact same parameters @96 epochs) will be loaded for ease of use\n",
    "\n",
    "#model.fit(train, validation_data=test, epochs=96, callbacks=[checkpoint_callback, schedule_callback, example_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef37bf-655a-4ab4-9c54-5075628bb118",
   "metadata": {},
   "source": [
    "##### Making Predictions With Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59390b64-3a26-4929-bdd9-47f1916c970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://drive.google.com/uc?id=1vWscXs4Vt0a_1IH1-ct2TCgXAZT-N3_Y'\n",
    "# checkpoints = 'checkpoints.zip'\n",
    "# gdown.download(url, checkpoints, quiet=False)\n",
    "# gdown.extractall('checkpoints.zip', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a02b0-2021-4651-9a0b-c348477f4811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('models/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739d3f0-db0d-4557-944f-f7906a9cdd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a4672-2297-4aac-8a35-af45fdf59edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture\n",
    "#take a sample video so we can make predictions on it\n",
    "sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750fa85-05cb-42c8-9c71-bf0ec95cb58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff87157-f53f-405b-a356-d1ec1dd54dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lay white sp by l eight please\n",
      "set white at i two please\n"
     ]
    }
   ],
   "source": [
    "#Real text of the video\n",
    "\n",
    "real_text_outputs = [tf.strings.reduce_join([num_char(word) for word in sentence]) for sentence in sample[1]]\n",
    "for output in real_text_outputs:\n",
    "    print(output.numpy().decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af554ddf-f5a4-4793-83f0-1cbe40ed1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lay white sp by l eight please\n",
      "set white at i two please\n"
     ]
    }
   ],
   "source": [
    "#decode predictions and print out what the model predicts\n",
    "decoded_text = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()\n",
    "predicted_text_output = [tf.strings.reduce_join([num_char(word) for word in sentence]) for sentence in decoded_text]\n",
    "for output in predicted_text_output:\n",
    "    print(output.numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9f314-fb7a-4767-8cbf-523f96b10c87",
   "metadata": {},
   "source": [
    "##### Test on Video from Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e7ef0-0505-40bd-95b2-cd841c6442ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sample = load_data(tf.convert_to_tensor('data/s1/lbad9a.mpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f242a9-3ce0-4807-b528-b16654202212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(tf.expand_dims(custom_sample[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab04fa-745c-4c3e-870d-0696726022fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL TEXT:\n",
      "lay blue at d nine again\n"
     ]
    }
   ],
   "source": [
    "print('REAL TEXT:')\n",
    "print([tf.strings.reduce_join([num_char(word) for word in sentence]) for sentence in [custom_sample[1]]][0].numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9bf18-413b-4c0a-942f-0a510da353e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION:\n",
      "lay blue at d nine again\n"
     ]
    }
   ],
   "source": [
    "print('PREDICTION:')\n",
    "decoded_text = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()\n",
    "predicted_text_output = [tf.strings.reduce_join([num_char(word) for word in sentence]) for sentence in decoded_text]\n",
    "print(predicted_text_output[0].numpy().decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
